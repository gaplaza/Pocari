{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooonsyk/Pocari/blob/main/classification/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikz7ZOGUrOMn",
        "outputId": "2af900e6-23ca-4149-819e-945a379e15c1"
      },
      "id": "ikz7ZOGUrOMn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **base model**"
      ],
      "metadata": {
        "id": "XusvRUeKynNA"
      },
      "id": "XusvRUeKynNA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9091d7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9091d7d",
        "outputId": "2f5a8c6d-2955-49b1-c44f-dfacd4030bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['blood', 'diarrhea', 'green', 'normal', 'white']\n",
            "Number of training samples: 27\n",
            "Number of validation samples: 10\n",
            "Number of test samples: 10\n"
          ]
        }
      ],
      "source": [
        "import os                                               #파일 접근\n",
        "import torch                                            #신경망 사용\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim                             #최적화 알고리즘 제공\n",
        "from torchvision import datasets, transforms            #데이터셋 사용, 이미지 데이터 전처리\n",
        "from torch.utils.data import DataLoader                 #데이터 미니배치로 나누어서 로드\n",
        "from sklearn.model_selection import train_test_split    #학습, 검증, 테스트 데이터 분리\n",
        "\n",
        "# 이미지 전처리 및 데이터 증강\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),          # 이미지 크기 조정\n",
        "    transforms.ToTensor(),                  # 이미지를 텐서로 변환\n",
        "    transforms.Normalize((0.5,), (0.5,))    # 정규화\n",
        "])\n",
        "\n",
        "# 이미지 데이터셋 로드\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/images_0601', transform=transform)\n",
        "\n",
        "# 데이터셋을 학습, 검증, 테스트로 분할\n",
        "train_val_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
        "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.25, random_state=42)  # train 60%, val 20%, test 20%\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "# DataLoader 생성 - data 수 적어서 batch 지정 안 함\n",
        "train_loader = DataLoader(dataset=train_dataset, shuffle=True)   # 오버피팅 방지, 일반화\n",
        "val_loader = DataLoader(dataset=val_dataset, shuffle=False)      # 일관성\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=False)\n",
        "\n",
        "# 데이터셋 클래스 확인\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(val_dataset))\n",
        "print(\"Number of test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d682775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d682775",
        "outputId": "ae03fd33-75fd-4a2e-a4d4-d0ebd7c872d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 1.7217538458881554\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch [2/10], Train Loss: 0.7891250360343192\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [3/10], Train Loss: 0.301701779782619\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch [4/10], Train Loss: 0.0685588334415612\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [5/10], Train Loss: 0.020239673532894433\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [6/10], Train Loss: 0.0009237195046580909\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [7/10], Train Loss: 0.0007025767062916404\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [8/10], Train Loss: 0.0005640616002269612\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [9/10], Train Loss: 0.00043823193444975713\n",
            "Validation Accuracy: 40.00%\n",
            "Epoch [10/10], Train Loss: 0.0003563228317983868\n",
            "Validation Accuracy: 40.00%\n"
          ]
        }
      ],
      "source": [
        "# 간단한 CNN 모델 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "    # 모델 구조 정의\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, len(dataset.classes))\n",
        "\n",
        "    # 데이터가 모델을 통과하는 과정 정의\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 32 * 32)    # 3D 텐서 1D텐서로 변환\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)                 # 최종 결과 출력\n",
        "        return x\n",
        "\n",
        "# 모델 초기화 및 손실 함수, 최적화 도구 설정\n",
        "model = SimpleCNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()                     # 다중 분류에 적합\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # 확률적 경사 하강법의 변형\n",
        "\n",
        "# 학습 및 검증 함수 정의\n",
        "def train(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()                         # 훈련모드\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:   # 미니배치를 가져와 입력과 레이블 받음\n",
        "            optimizer.zero_grad()             # 이전에 계산한 그래디언트 초기화\n",
        "            outputs = model(inputs)           # 결과 출력\n",
        "            loss = criterion(outputs, labels) # 손실 계산\n",
        "            loss.backward()                   # 역전파로 그래디언트 업데이트\n",
        "            optimizer.step()                  # 파라미터 업데이트\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # 학습 중 손실 출력\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "        # 검증 데이터셋을 사용하여 모델 성능 평가\n",
        "        model.eval()                          # 평가모드\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():                 # 그래디언트 계산 진행하지 않음\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)                  # 결과 출력\n",
        "                _, predicted = torch.max(outputs, 1)     # 각 데이터에 대해 예측된 레이블\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_accuracy = correct / total\n",
        "        print(f'Validation Accuracy: {val_accuracy:.2%}')\n",
        "\n",
        "# 모델 학습\n",
        "train(model, criterion, optimizer, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93339ece",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93339ece",
        "outputId": "cee06a77-b70d-4e78-bdbd-255ac1810685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.00%\n",
            "True Labels: [3, 4, 3, 4, 3, 4, 2, 3, 1, 3]\n",
            "Predicted Labels: [0, 4, 3, 4, 3, 4, 2, 3, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터셋을 사용하여 모델 평가\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    #블록 내에서 테스트 데이터로더를 반복하면서 각 미니배치에 대한 예측을 수행\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # 예측된 레이블과 실제 레이블 저장\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f'Test Accuracy: {test_accuracy:.2%}')\n",
        "    return true_labels, predicted_labels\n",
        "\n",
        "true_labels, predicted_labels = test(model, test_loader)\n",
        "\n",
        "# 실제 레이블과 예측된 레이블 출력\n",
        "print(\"True Labels:\", true_labels)\n",
        "print(\"Predicted Labels:\", predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8bc45b",
      "metadata": {
        "id": "df8bc45b"
      },
      "source": [
        "## **white balance 전처리 추가**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd74523",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bd74523",
        "outputId": "08327567-1409-4722-c7fa-f44ed5399a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['blood', 'diarrhea', 'green', 'normal', 'white']\n",
            "Number of training samples: 27\n",
            "Number of validation samples: 10\n",
            "Number of test samples: 10\n"
          ]
        }
      ],
      "source": [
        "import os                                               #파일 접근\n",
        "import torch                                            #신경망 사용\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim                             #최적화 알고리즘 제공\n",
        "from torchvision import datasets, transforms            #데이터셋 사용, 이미지 데이터 전처리\n",
        "from torch.utils.data import DataLoader                 #데이터 미니배치로 나누어서 로드\n",
        "from sklearn.model_selection import train_test_split    #학습, 검증, 테스트 데이터 분리\n",
        "from skimage import img_as_ubyte                        #이미지 처리 - white balance 에 활용\n",
        "from PIL import Image\n",
        "\n",
        "def white_patch(image, percentile=100):\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : 입력으로 받는 이미지는 RGB 채널을 각각 가지는 (height, width, channels) 3차원 numpy 배열\n",
        "    percentile : 채널 값의 보정값으로 사용하고자 하는 값을 지정하기 위한 비율, 기본값은 100으로 최대값을 사용\n",
        "    \"\"\"\n",
        "\n",
        "    image = np.array(image)\n",
        "\n",
        "    white_patch_image = img_as_ubyte(\n",
        "        (image * 1.0 / np.percentile(image,\n",
        "                                     percentile,\n",
        "                                     axis=(0, 1))).clip(0, 1))\n",
        "    return Image.fromarray(white_patch_image)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.Lambda(lambda x: white_patch(x, percentile=85)),  # 전처리 함수 적용\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # 정규화\n",
        "])\n",
        "\n",
        "# 이미지 데이터셋 로드\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/images_0601', transform=transform)\n",
        "\n",
        "# 데이터셋을 학습, 검증, 테스트로 분할\n",
        "train_val_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
        "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.25, random_state=42)  # train 60%, val 20%, test 20%\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "# DataLoader 생성 - data 수 적어서 batch 지정 안 함\n",
        "train_loader = DataLoader(dataset=train_dataset, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=False)\n",
        "\n",
        "# 데이터셋 클래스 확인\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(val_dataset))\n",
        "print(\"Number of test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdb0ec1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cdb0ec1",
        "outputId": "5fd5fd15-fd7c-4de4-d0b1-4c35ccd16dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 2.167915595160728\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch [2/10], Train Loss: 1.2100594691173345\n",
            "Validation Accuracy: 20.00%\n",
            "Epoch [3/10], Train Loss: 0.6055220785136852\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [4/10], Train Loss: 0.10382025867241922\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [5/10], Train Loss: 0.014618972726094836\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [6/10], Train Loss: 0.004240569718797781\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [7/10], Train Loss: 0.0014227716425239123\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [8/10], Train Loss: 0.0008523498988526828\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [9/10], Train Loss: 0.0005332201536650364\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch [10/10], Train Loss: 0.0003883686386948347\n",
            "Validation Accuracy: 50.00%\n"
          ]
        }
      ],
      "source": [
        "# 간단한 CNN 모델 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 50 * 50, 128)  # 이 부분을 수정하여 두 번째 Conv 레이어의 출력 크기에 맞춰야 합니다.\n",
        "        self.fc2 = nn.Linear(128, len(dataset.classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 50 * 50)  # 두 번째 Conv 레이어를 통과한 후의 출력 크기에 맞추어 수정합니다.\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화 및 손실 함수, 최적화 도구 설정\n",
        "model = SimpleCNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 및 검증 함수 정의\n",
        "def train(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # 학습 중 손실 출력\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "        # 검증 데이터셋을 사용하여 모델 성능 평가\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_accuracy = correct / total\n",
        "        print(f'Validation Accuracy: {val_accuracy:.2%}')\n",
        "\n",
        "# 모델 학습\n",
        "train(model, criterion, optimizer, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c75ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6c75ec",
        "outputId": "86f92347-34a2-46a2-fbd9-721a63761280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.00%\n",
            "True Labels: [3, 4, 3, 4, 3, 4, 2, 3, 1, 3]\n",
            "Predicted Labels: [3, 4, 3, 4, 2, 4, 2, 3, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터셋을 사용하여 모델 평가\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # 예측된 레이블과 실제 레이블 저장\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f'Test Accuracy: {test_accuracy:.2%}')\n",
        "    return true_labels, predicted_labels\n",
        "\n",
        "true_labels, predicted_labels = test(model, test_loader)\n",
        "\n",
        "# 실제 레이블과 예측된 레이블 출력\n",
        "print(\"True Labels:\", true_labels)\n",
        "print(\"Predicted Labels:\", predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7c9f37",
      "metadata": {
        "id": "db7c9f37"
      },
      "source": [
        "### **이미지 증강 추가**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d548f564",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d548f564",
        "outputId": "7652684f-7030-4b2a-9ac9-d242c049230f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['blood', 'diarrhea', 'green', 'normal', 'white']\n",
            "Number of training samples: 27\n",
            "Number of validation samples: 10\n",
            "Number of test samples: 10\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import img_as_ubyte\n",
        "from PIL import Image\n",
        "\n",
        "def white_patch(image, percentile=100):\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : 입력으로 받는 이미지는 RGB 채널을 각각 가지는 (height, width, channels) 3차원 numpy 배열\n",
        "    percentile : 채널 값의 보정값으로 사용하고자 하는 값을 지정하기 위한 비율, 기본값은 100으로 최대값을 사용\n",
        "    \"\"\"\n",
        "\n",
        "    image = np.array(image)\n",
        "\n",
        "    white_patch_image = img_as_ubyte(\n",
        "        (image * 1.0 / np.percentile(image,\n",
        "                                     percentile,\n",
        "                                     axis=(0, 1))).clip(0, 1))\n",
        "    return Image.fromarray(white_patch_image)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.Lambda(lambda x: white_patch(x, percentile=85)),  # 전처리 함수 적용\n",
        "    transforms.RandomHorizontalFlip(),  # 랜덤으로 이미지를 수평으로 뒤집기\n",
        "    transforms.RandomVerticalFlip(p=0.5), # 랜덤으로 이미지를 수직으로 뒤집기\n",
        "    transforms.RandomRotation(degrees=(-10, 10)), # 랜덤으로 이미지 회전\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # 정규화\n",
        "])\n",
        "\n",
        "# 이미지 데이터셋 로드\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/images_0601', transform=transform)\n",
        "\n",
        "# 데이터셋을 학습, 검증, 테스트로 분할\n",
        "train_val_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=1)\n",
        "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.25, random_state=1)  # train 60%, val 20%, test 20%\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "# DataLoader 생성 - data 수 적어서 batch 지정 안 함\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size = 1, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size = 1, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size = 1, shuffle=False)\n",
        "\n",
        "# 데이터셋 클래스 확인\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(val_dataset))\n",
        "print(\"Number of test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b42a9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34b42a9f",
        "outputId": "01599830-7cfe-4fcc-dad8-87fa2e954fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 2.676086973674871\n",
            "Validation Accuracy: 80.00%\n",
            "Epoch [2/10], Train Loss: 1.341817690818398\n",
            "Validation Accuracy: 80.00%\n",
            "Epoch [3/10], Train Loss: 1.148970368663194\n",
            "Validation Accuracy: 90.00%\n",
            "Epoch [4/10], Train Loss: 1.248877362796554\n",
            "Validation Accuracy: 90.00%\n",
            "Epoch [5/10], Train Loss: 1.048879081348854\n",
            "Validation Accuracy: 80.00%\n",
            "Epoch [6/10], Train Loss: 1.13217444485269\n",
            "Validation Accuracy: 90.00%\n",
            "Epoch [7/10], Train Loss: 0.8422969690366665\n",
            "Validation Accuracy: 90.00%\n",
            "Epoch [8/10], Train Loss: 0.6591349620939582\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch [9/10], Train Loss: 0.6768748494954269\n",
            "Validation Accuracy: 70.00%\n",
            "Epoch [10/10], Train Loss: 0.7099919161603886\n",
            "Validation Accuracy: 90.00%\n"
          ]
        }
      ],
      "source": [
        "# 간단한 CNN 모델 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 50 * 50, 128)\n",
        "        self.fc2 = nn.Linear(128, len(dataset.classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 50 * 50)  # 두 번째 Conv 레이어를 통과한 후의 출력 크기에 맞추어 수정합니다.\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화 및 손실 함수, 최적화 도구 설정\n",
        "model = SimpleCNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 및 검증 함수 정의\n",
        "def train(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # 학습 중 손실 출력\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "        # 검증 데이터셋을 사용하여 모델 성능 평가\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_accuracy = correct / total\n",
        "        print(f'Validation Accuracy: {val_accuracy:.2%}')\n",
        "\n",
        "# 모델 학습\n",
        "train(model, criterion, optimizer, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8850e689",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8850e689",
        "outputId": "8c77e96a-68b3-41ba-958c-374fbf153562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 40.00%\n",
            "True Labels: [3, 3, 4, 4, 0, 1, 3, 3, 4, 3]\n",
            "Predicted Labels: [4, 1, 4, 2, 3, 4, 3, 3, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터셋을 사용하여 모델 평가\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # 예측된 레이블과 실제 레이블 저장\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f'Test Accuracy: {test_accuracy:.2%}')\n",
        "    return true_labels, predicted_labels\n",
        "\n",
        "true_labels, predicted_labels = test(model, test_loader)\n",
        "\n",
        "# 실제 레이블과 예측된 레이블 출력\n",
        "print(\"True Labels:\", true_labels)\n",
        "print(\"Predicted Labels:\", predicted_labels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}